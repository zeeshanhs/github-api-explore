{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Github API\n",
    "\n",
    "<br />\n",
    "\n",
    "Usage demonstration of some base python functionalities to compute results on data from API.\n",
    "\n",
    "<br />\n",
    "\n",
    "### Computed results:\n",
    "1. Total stargazers count per language\n",
    "2. Average Stargazers-count per Language\n",
    "3. Language-wise ratio of forks to watchers and individual averages of each\n",
    "4. 'Repo Bigness' and its effect on %age contribution to total watchers of its respective language\n",
    "\n",
    "<br />\n",
    "\n",
    "### Key highlights:\n",
    "1. Approach\n",
    "2. Documentation\n",
    "3. Attention to forward compatibility\n",
    "4. (one of the) Simple solution to **_Windowing_** problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received\n",
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "from itertools import groupby\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "URL = 'http://api.github.com/orgs/zendesk/repos'\n",
    "\n",
    "r = requests.get(URL)\n",
    "\n",
    "if r.status_code == 200:\n",
    "    print ('Response received')\n",
    "else: print ('Failure')\n",
    "\n",
    "if 'json' in r.headers.get('Content-Type'):\n",
    "    data = r.json()\n",
    "    print ('Data loaded successfully')\n",
    "else:\n",
    "    print('Response content is not in JSON format.')\n",
    "    data = 'spam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total stargazers count per language\n",
    "\n",
    "<br />\n",
    "\n",
    "Basic summation of stargazers on each language. The solution makes use of:\n",
    "\n",
    "- filter\n",
    "- sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 repos in 'RUBY'\n",
      "--------\n",
      "('dropbox-api', 357)\n",
      "('zendesk_api_client_rb', 340)\n",
      "('arturo', 164)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# total stargazers_count per language\n",
    "keys_of_interest = {'name', 'language', 'stargazers_count'}\n",
    "\n",
    "# Ensure all keys exist - Discard rows that don't have required values\n",
    "removeInvalids = filter(lambda dct: keys_of_interest.issubset(set(dct.keys())), data) # lazy evaluation\n",
    "\n",
    "# project dict to tuple with required columns/keys for easy groupby operation\n",
    "#---NOTE: Sorted keys_of_interest to ensure that order of result doesnt change\n",
    "sorted_KOI = sorted(keys_of_interest) # pre-computed to avoid calls for all rows\n",
    "projectedData = (tuple(dct[k] for k in sorted_KOI) for dct in removeInvalids) #generator\n",
    "\n",
    "\n",
    "# Remove language info as we know resultset is only for 'Ruby'\n",
    "def filter_language(inpData, lang):\n",
    "    return [(r[1], r[2]) for r in (filter(lambda tp: tp[0] == lang, inpData))] # tp[0] = language\n",
    "\n",
    "# Lazy map till filter and then sorted computes\n",
    "res = sorted(filter_language(projectedData, 'Ruby'), key=lambda tp: tp[1], reverse=True)\n",
    "\n",
    "# Clean printing\n",
    "print ('Top 3 repos in \\'RUBY\\'')\n",
    "print ('--------')\n",
    "for r in res[:3]:\n",
    "    print (r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Stargazers-count per Language\n",
    "\n",
    "A basic calculation of average stargazers-count per language and then display top 3 languages as result. Formula for calculation is given below:\n",
    "\n",
    "> formula of average stargazers per language = (total stargazers for language)/(repo count of language)\n",
    "\n",
    "<br />\n",
    "\n",
    "### Key areas:\n",
    "\n",
    "1. Kept \"keys_of_interest\" as set to use 'issubset' - optimization for removing invalids (for demonstration only)\n",
    "2. Projected data from dict to tuple - remove unnecessary data\n",
    "3. column_wise_sum(): removes hardcoding of which columns to sum. Also unifies logic in 1 place for exception handling. Supports any number of columns to find creative summations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average stargazers for language 'Ruby' are: 47.69\n",
      "Average stargazers for language 'JavaScript' are: 1.0\n",
      "Average stargazers for language 'Java' are: 13.0\n",
      "Average stargazers for language 'C#' are: 8.0\n",
      "Average stargazers for language 'C' are: 4.0\n"
     ]
    }
   ],
   "source": [
    "# Using same projected data from previously but adding a variation for COUNT\n",
    "\n",
    "keys_of_interest = {'name', 'language', 'stargazers_count'}\n",
    "# Ensure all keys exist - Discard rows that don't have required values\n",
    "removeInvalids = filter(lambda dct: keys_of_interest.issubset(set(dct.keys())), data) # lazy evaluation\n",
    "\n",
    "sorted_KOI = sorted(keys_of_interest) # pre-computed to avoid calls for all rows\n",
    "# project dict to tuple with required columns/keys for easy groupby operation\n",
    "#---NOTE: Sorted keys_of_interest to ensure that order of result doesnt change\n",
    "#------ Added extra value of '1' to tuple for using as COUNTER\n",
    "projectedData = (tuple(dct[k] for k in sorted_KOI) + (1,) for dct in removeInvalids)\n",
    "\n",
    "# sort descending by language and stargazer-count\n",
    "sortedData = sorted(projectedData, key=lambda k: (k[0], k[2]), reverse=True) # Groupby pre-requisite\n",
    "\n",
    "\n",
    "def column_wise_sum (grpList, column_ids):\n",
    "    # Input: list of tuples and list of id's of columns to sum.\n",
    "    inpList = list(grpList) # groupby object conversion\n",
    "    tuple_length = len(inpList[0]) # pre-compute to save cost\n",
    "    # subset data, i.e., extract numeric columns to apply custom logic on.\n",
    "    subset = [tuple(tp[i] for i in column_ids if i < tuple_length) for tp in inpList]\n",
    "    \n",
    "    \"\"\"\n",
    "    Logic: Expand and zip the list of tuples to convert it into column-wise list. Transpose. e.g.,\n",
    "        [(2, 6), (7, 9)] => [(2, 7), (6, 9)]\n",
    "           Then use SUM on each list to compute total\n",
    "    \"\"\"\n",
    "    return tuple(sum(z) for z in zip(*subset))\n",
    "    \n",
    "# Use custom function to compute sum of each individual column, here: total-stargazers and repo-count\n",
    "# Result = Language-wise total stargazers and repository count\n",
    "groupedTotal = [ (k,) + (column_wise_sum(grp, [2, 3])) \\\n",
    "                        for k, grp in groupby(sortedData, key=lambda t: t[0])]\n",
    "\n",
    "\n",
    "# Computing average stargazers per language\n",
    "for tp in groupedTotal:\n",
    "    print (\"Average stargazers for language \\'{}\\' are: {}\".format(tp[0], round(tp[1] / tp[2], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language-wise ratio of forks to watchers and individual averages of each\n",
    "\n",
    "Refer to the ratio of forks to watchers as 'Conversion'.\n",
    "\n",
    "<br />\n",
    "\n",
    "This is really do demonstrate the re-usability of the approach I used in earlier example. Only data projection step will differ and final formula since we are calculating on multiple columns but rest stays the same as it is forward compatible.\n",
    "\n",
    "This is usually my preferred approach of coding. Keep things nicely atomic so changes are centralized. And keep Tier 1, 2, and 3 functions where T1 are atomic functions, T2 uses T1 functions, and T3 is mostly pipeline structured that group multiple operations together.\n",
    "\n",
    "> column_wise_sum() function will really come in handy here as we will be calculating sum of 5 columns at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'Ruby' language -- Repo count = 26 ====> conversion: 26.77% ~~~ Average Forks: 12.77 and Average Watchers: 47.69\n",
      "For 'JavaScript' language -- Repo count = 1 ====> conversion: 0.0% ~~~ Average Forks: 0.0 and Average Watchers: 1.0\n",
      "For 'Java' language -- Repo count = 1 ====> conversion: 53.85% ~~~ Average Forks: 7.0 and Average Watchers: 13.0\n",
      "For 'C#' language -- Repo count = 1 ====> conversion: 12.5% ~~~ Average Forks: 1.0 and Average Watchers: 8.0\n",
      "For 'C' language -- Repo count = 1 ====> conversion: 50.0% ~~~ Average Forks: 2.0 and Average Watchers: 4.0\n"
     ]
    }
   ],
   "source": [
    "# Using same projected data from previously but adding a variation for COUNT\n",
    "\n",
    "keys_of_interest = {'name', 'language', 'forks', 'watchers'}\n",
    "# Ensure all keys exist - Discard rows that don't have required values\n",
    "removeInvalids = filter(lambda dct: keys_of_interest.issubset(set(dct.keys())), data) # lazy evaluation\n",
    "\n",
    "sorted_KOI = ['name', 'language', 'forks', 'watchers'] # desired order of columns for our computation\n",
    "# project dict to tuple with required columns/keys for easy groupby operation\n",
    "#---NOTE: Sorted keys_of_interest to ensure that order of result doesnt change\n",
    "#------ Added extra value of '1' to tuple for using as COUNTER\n",
    "projectedData = (tuple(dct[k] for k in sorted_KOI) + (1,) for dct in removeInvalids)\n",
    "\n",
    "# sort descending by language (column 1) and watchers (column 3)\n",
    "sortedData = sorted(projectedData, key=lambda k: (k[1], k[3]), reverse=True) # Groupby pre-requisite\n",
    "\n",
    "def column_wise_sum (grpList, column_ids):\n",
    "    # Input: list of tuples and list of id's of columns to sum.\n",
    "    if isinstance(grpList, list):\n",
    "        print (\"its already list\")\n",
    "        inpList = grpList\n",
    "    else: inpList = list(grpList) # groupby object conversion\n",
    "    \n",
    "    tuple_length = len(inpList[0]) # pre-compute to save cost\n",
    "    # subset data, i.e., extract numeric columns to apply custom logic on.\n",
    "    subset = [tuple(tp[i] for i in column_ids if i < tuple_length) for tp in inpList]\n",
    "    \n",
    "    \"\"\"\n",
    "    Logic: Expand and zip the list of tuples to convert it into column-wise list. Transpose. e.g.,\n",
    "        [(2, 6), (7, 9)] => [(2, 7), (6, 9)]\n",
    "           Then use SUM on each list to compute total\n",
    "    \"\"\"\n",
    "    return tuple(sum(z) for z in zip(*subset))\n",
    "    \n",
    "# Use custom function to compute sum of each individual column, here: total-stargazers and repo-count\n",
    "# Result = Language-wise total stargazers and repository count\n",
    "groupedTotal = [ (k,) + (column_wise_sum(grp, [2, 3, 4])) \\\n",
    "                        for k, grp in groupby(sortedData, key=lambda t: t[1])] # groupby LANGUAGE\n",
    "\n",
    "\n",
    "# Computing Language-wise ratio of forks to watchers and individual averages of each\n",
    "# conversion = ratio of forks to watchers.\n",
    "for tp in groupedTotal:\n",
    "    print (\"For \\'{}\\' language -- Repo count = {} ====> conversion: {}% ~~~ Average Forks: {} and Average Watchers: {}\" \\\n",
    "        .format(tp[0], tp[3], round((tp[1] / tp[2])*100, 2), round(tp[1] / tp[3], 2), round(tp[2] / tp[3], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display 'bigness' in descending order of the %age contribution to total watchers of its respective language\n",
    "\n",
    "<br />\n",
    "\n",
    "Computed an arbitrary column 'bigness' for demonstration; categorized repos to 'small', 'medium', and 'large' on the basis of their 'size' attribute. The idea was to compute two aggregates with different grain on data in efficient way, i.e., one grouping over 'bigness' and other to calculate total over 'language'.\n",
    "\n",
    "Suggested methodology computes aggregates in O(N) on a generator function and then uses the dictionary representation for data that is easy to display output with.\n",
    "\n",
    "<br />\n",
    "\n",
    "Operation looks like:\n",
    "\n",
    "\n",
    "```\n",
    "e.g.,\n",
    "Last 2 columns are computed\n",
    "\n",
    "bigness      | watchers  | language  | language_watchers  | bigness_contribution\n",
    "................................................................................\n",
    "medium       | 200       | Python    | 500                | 40%\n",
    "small        | 300       | Python    | 500                | 60%\n",
    "medium       | 150       | Ruby      | 200                | 75%\n",
    "large        | 50        | Ruby      | 200                | 25%\n",
    "\n",
    "\n",
    "RESULT:\n",
    "bigness    | watchers  | language  | language_watchers  | bigness_contribution\n",
    "..............................................................................\n",
    "medium     | 150       | Ruby      | 200                | 75%\n",
    "small      | 300       | Python    | 500                | 60%\n",
    "medium     | 200       | Python    | 500                | 40%\n",
    "large      | 50        | Ruby      | 200                | 25%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'bigness' in descending order of the %age contribution to total watchers of its respective language\n",
      "-----------\n",
      "{'bigness': 'medium', 'watchers': 13, 'language': 'Java', 'language_watchers': 13, 'bigness_contribution': 100.0}\n",
      "{'bigness': 'small', 'watchers': 8, 'language': 'C#', 'language_watchers': 8, 'bigness_contribution': 100.0}\n",
      "{'bigness': 'large', 'watchers': 1, 'language': 'JavaScript', 'language_watchers': 1, 'bigness_contribution': 100.0}\n",
      "{'bigness': 'large', 'watchers': 4, 'language': 'C', 'language_watchers': 4, 'bigness_contribution': 100.0}\n",
      "{'bigness': 'medium', 'watchers': 650, 'language': 'Ruby', 'language_watchers': 1240, 'bigness_contribution': 52.42}\n",
      "{'bigness': 'large', 'watchers': 524, 'language': 'Ruby', 'language_watchers': 1240, 'bigness_contribution': 42.26}\n",
      "{'bigness': 'small', 'watchers': 66, 'language': 'Ruby', 'language_watchers': 1240, 'bigness_contribution': 5.32}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict # to avoid initializations of dictionary\n",
    "\n",
    "# keys_of_interest = {'name', 'language', 'watchers'}\n",
    "# removeInvalids = filter(lambda dct: keys_of_interest.issubset(set(dct.keys())), data)\n",
    "dataNew = data\n",
    "# Adding a new computed column\n",
    "for dct in dataNew:\n",
    "    if dct['size'] <= 100:\n",
    "        dct['bigness'] = 'small'\n",
    "    elif dct['size'] > 100 and dct['size'] <= 500:\n",
    "        dct['bigness'] = 'medium'\n",
    "    else: dct['bigness'] = 'large'\n",
    "\n",
    "sorted_KOI = ['bigness', 'language', 'watchers'] # desired order of columns for our computation\n",
    "# project dict to tuple with required columns/keys for easy groupby operation\n",
    "#------ Added extra value of '1' to tuple for using as COUNTER\n",
    "projectedData = (tuple(dct[k] for k in sorted_KOI) + (1,) for dct in dataNew)\n",
    "\n",
    "# Avoiding the need to manually initalize 'expected keys' in advance\n",
    "aggResult = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# leverage power of generator and simplistic approach using dictionary for aggregates\n",
    "for row in projectedData:\n",
    "    groupByKey = row[0] # main aggregation over 'bigness' column\n",
    "    windowKey = row[1] # windowing over language\n",
    "    aggResult[groupByKey][windowKey] += row[2] # add watchers to respective language\n",
    "\n",
    "# Expanding nested and repeated format. Calculate window aggregate at same time. \n",
    "P4resultSet = []\n",
    "aggWindow = defaultdict(int) # hold totals for each language\n",
    "for k,v in aggResult.items():\n",
    "    for ik in v.keys():\n",
    "        P4resultSet.append({'bigness' : k, 'watchers' : v[ik], 'language' : ik}) # modifying key\n",
    "        aggWindow[ik] += v[ik]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Apply final transformation to JOIN two aggregates over 'language' column and compute \n",
    "    contribution percentage\n",
    "\"\"\"\n",
    "for dct in P4resultSet:\n",
    "    lang_total = aggWindow[dct['language']]\n",
    "    bigness_contrib = round((dct['watchers'] / lang_total) * 100, 2)\n",
    "    dct.update({'language_watchers' : lang_total, 'bigness_contribution' : bigness_contrib})\n",
    "    \n",
    "print (\"\\'bigness\\' in descending order of the %age contribution to total watchers of its respective language\")\n",
    "print ('-----------')\n",
    "finalResP4 = sorted(P4resultSet, key=lambda d: d['bigness_contribution'], reverse=True)\n",
    "for r in finalResP4: # clean printing\n",
    "    print (r)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
